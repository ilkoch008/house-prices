{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "import os\n",
    "import wget\n",
    "from pathlib import Path\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "train = pd.read_csv(filepath_or_buffer='train.csv')\n",
    "target = 'SalePrice'\n",
    "if \"Set\" not in train.columns:\n",
    "    train[\"Set\"] = np.random.choice([\"train\", \"valid\", \"test\"], p =[.9, 0.05, 0.05], size=(train.shape[0],))\n",
    "\n",
    "train_indices = train[train.Set==\"train\"].index\n",
    "valid_indices = train[train.Set==\"valid\"].index\n",
    "test_indices = train[train.Set==\"test\"].index"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSZoning 5\n",
      "Street 2\n",
      "Alley 2\n",
      "LotShape 4\n",
      "LandContour 4\n",
      "Utilities 2\n",
      "LotConfig 5\n",
      "LandSlope 3\n",
      "Neighborhood 25\n",
      "Condition1 9\n",
      "Condition2 8\n",
      "BldgType 5\n",
      "HouseStyle 8\n",
      "RoofStyle 6\n",
      "RoofMatl 8\n",
      "Exterior1st 15\n",
      "Exterior2nd 16\n",
      "MasVnrType 4\n",
      "ExterQual 4\n",
      "ExterCond 5\n",
      "Foundation 6\n",
      "BsmtQual 4\n",
      "BsmtCond 4\n",
      "BsmtExposure 4\n",
      "BsmtFinType1 6\n",
      "BsmtFinType2 6\n",
      "Heating 6\n",
      "HeatingQC 5\n",
      "CentralAir 2\n",
      "Electrical 5\n",
      "KitchenQual 4\n",
      "Functional 7\n",
      "FireplaceQu 5\n",
      "GarageType 6\n",
      "GarageFinish 3\n",
      "GarageQual 5\n",
      "GarageCond 5\n",
      "PavedDrive 3\n",
      "PoolQC 3\n",
      "Fence 4\n",
      "MiscFeature 4\n",
      "SaleType 9\n",
      "SaleCondition 6\n",
      "Set 3\n"
     ]
    }
   ],
   "source": [
    "categorical_columns = []\n",
    "categorical_dims =  {}\n",
    "for col in train.columns[train.dtypes == object]:\n",
    "    print(col, train[col].nunique())\n",
    "    l_enc = LabelEncoder()\n",
    "    train[col] = train[col].fillna(\"VV_likely\")\n",
    "    train[col] = l_enc.fit_transform(train[col].values)\n",
    "    categorical_columns.append(col)\n",
    "    categorical_dims[col] = len(l_enc.classes_)\n",
    "\n",
    "for col in train.columns[train.dtypes == 'float64']:\n",
    "    train.fillna(train.loc[train_indices, col].mean(), inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "unused_feat = ['Id']\n",
    "\n",
    "features = [ col for col in train.columns if col not in unused_feat+[target]]\n",
    "\n",
    "cat_idxs = [ i for i, f in enumerate(features) if f in categorical_columns]\n",
    "\n",
    "cat_dims = [ categorical_dims[f] for i, f in enumerate(features) if f in categorical_columns]\n",
    "\n",
    "# print(cat_dims)\n",
    "\n",
    "# define your embedding sizes : here just a random choice\n",
    "# cat_emb_dim = [5, 4, 3, 6, 2, 2, 1, 10]\n",
    "cat_emb_dim = (np.ones(len(cat_dims))*100).astype(np.int)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda\n"
     ]
    }
   ],
   "source": [
    "clf = TabNetRegressor(cat_dims=cat_dims, cat_emb_dim=cat_emb_dim, cat_idxs=cat_idxs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "X_train = train[features].values[train_indices]\n",
    "y_train = train[target].values[train_indices].reshape(-1, 1)/1000000\n",
    "\n",
    "X_valid = train[features].values[valid_indices]\n",
    "y_valid = train[target].values[valid_indices].reshape(-1, 1)/1000000\n",
    "\n",
    "X_test = train[features].values[test_indices]\n",
    "y_test = train[target].values[test_indices].reshape(-1, 1)/1000000"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "max_epochs = 300 if not os.getenv(\"CI\", False) else 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.73285 | train_mae: 0.17449 | train_mse: 0.03836 | valid_mae: 0.19546 | valid_mse: 0.04843 |  0:00:01s\n",
      "epoch 1  | loss: 0.38767 | train_mae: 0.21083 | train_mse: 0.11176 | valid_mae: 0.20842 | valid_mse: 0.05219 |  0:00:01s\n",
      "epoch 2  | loss: 0.30598 | train_mae: 0.17737 | train_mse: 0.03833 | valid_mae: 0.1948  | valid_mse: 0.0469  |  0:00:02s\n",
      "epoch 3  | loss: 0.17778 | train_mae: 0.15123 | train_mse: 0.02921 | valid_mae: 0.16537 | valid_mse: 0.03602 |  0:00:02s\n",
      "epoch 4  | loss: 0.09332 | train_mae: 0.13332 | train_mse: 0.02474 | valid_mae: 0.1474  | valid_mse: 0.03029 |  0:00:03s\n",
      "epoch 5  | loss: 0.08375 | train_mae: 0.12866 | train_mse: 0.02354 | valid_mae: 0.14271 | valid_mse: 0.02959 |  0:00:03s\n",
      "epoch 6  | loss: 0.05585 | train_mae: 0.13281 | train_mse: 0.02477 | valid_mae: 0.14691 | valid_mse: 0.03048 |  0:00:04s\n",
      "epoch 7  | loss: 0.0484  | train_mae: 0.13336 | train_mse: 0.0251  | valid_mae: 0.14637 | valid_mse: 0.02945 |  0:00:04s\n",
      "epoch 8  | loss: 0.0361  | train_mae: 0.10862 | train_mse: 0.01927 | valid_mae: 0.11417 | valid_mse: 0.02092 |  0:00:05s\n",
      "epoch 9  | loss: 0.02495 | train_mae: 0.10071 | train_mse: 0.01686 | valid_mae: 0.10715 | valid_mse: 0.01882 |  0:00:05s\n",
      "epoch 10 | loss: 0.01677 | train_mae: 0.10599 | train_mse: 0.01777 | valid_mae: 0.12339 | valid_mse: 0.02201 |  0:00:06s\n",
      "epoch 11 | loss: 0.01497 | train_mae: 0.10212 | train_mse: 0.01616 | valid_mae: 0.12123 | valid_mse: 0.02122 |  0:00:06s\n",
      "epoch 12 | loss: 0.01491 | train_mae: 0.07635 | train_mse: 0.01041 | valid_mae: 0.0894  | valid_mse: 0.01405 |  0:00:06s\n",
      "epoch 13 | loss: 0.01162 | train_mae: 0.06101 | train_mse: 0.0071  | valid_mae: 0.0651  | valid_mse: 0.0084  |  0:00:07s\n",
      "epoch 14 | loss: 0.01083 | train_mae: 0.06138 | train_mse: 0.00692 | valid_mae: 0.05816 | valid_mse: 0.00722 |  0:00:07s\n",
      "epoch 15 | loss: 0.00943 | train_mae: 0.06417 | train_mse: 0.0077  | valid_mae: 0.06749 | valid_mse: 0.00913 |  0:00:08s\n",
      "epoch 16 | loss: 0.00873 | train_mae: 0.06921 | train_mse: 0.00867 | valid_mae: 0.07564 | valid_mse: 0.01028 |  0:00:08s\n",
      "epoch 17 | loss: 0.00846 | train_mae: 0.06513 | train_mse: 0.0076  | valid_mae: 0.06283 | valid_mse: 0.00775 |  0:00:09s\n",
      "epoch 18 | loss: 0.00777 | train_mae: 0.06765 | train_mse: 0.00777 | valid_mae: 0.06477 | valid_mse: 0.00765 |  0:00:09s\n",
      "epoch 19 | loss: 0.00751 | train_mae: 0.06818 | train_mse: 0.00804 | valid_mae: 0.07762 | valid_mse: 0.01    |  0:00:10s\n",
      "epoch 20 | loss: 0.00726 | train_mae: 0.0644  | train_mse: 0.00752 | valid_mae: 0.07312 | valid_mse: 0.00986 |  0:00:10s\n",
      "epoch 21 | loss: 0.00727 | train_mae: 0.06499 | train_mse: 0.00702 | valid_mae: 0.06663 | valid_mse: 0.00802 |  0:00:11s\n",
      "epoch 22 | loss: 0.00671 | train_mae: 0.07663 | train_mse: 0.00868 | valid_mae: 0.07837 | valid_mse: 0.00951 |  0:00:11s\n",
      "epoch 23 | loss: 0.00662 | train_mae: 0.06444 | train_mse: 0.00688 | valid_mae: 0.06664 | valid_mse: 0.00796 |  0:00:11s\n",
      "epoch 24 | loss: 0.00636 | train_mae: 0.06075 | train_mse: 0.00669 | valid_mae: 0.06175 | valid_mse: 0.00794 |  0:00:12s\n",
      "epoch 25 | loss: 0.00647 | train_mae: 0.07549 | train_mse: 0.00862 | valid_mae: 0.07525 | valid_mse: 0.00901 |  0:00:12s\n",
      "epoch 26 | loss: 0.00601 | train_mae: 0.08082 | train_mse: 0.00942 | valid_mae: 0.0791  | valid_mse: 0.00952 |  0:00:13s\n",
      "epoch 27 | loss: 0.00581 | train_mae: 0.06389 | train_mse: 0.00687 | valid_mae: 0.0625  | valid_mse: 0.00742 |  0:00:13s\n",
      "epoch 28 | loss: 0.00576 | train_mae: 0.0592  | train_mse: 0.00624 | valid_mae: 0.05689 | valid_mse: 0.00696 |  0:00:14s\n",
      "epoch 29 | loss: 0.00599 | train_mae: 0.0667  | train_mse: 0.00702 | valid_mae: 0.06638 | valid_mse: 0.0078  |  0:00:14s\n",
      "epoch 30 | loss: 0.00562 | train_mae: 0.06487 | train_mse: 0.00676 | valid_mae: 0.06331 | valid_mse: 0.00735 |  0:00:15s\n",
      "epoch 31 | loss: 0.00552 | train_mae: 0.05807 | train_mse: 0.00602 | valid_mae: 0.05875 | valid_mse: 0.00723 |  0:00:15s\n",
      "epoch 32 | loss: 0.00556 | train_mae: 0.05628 | train_mse: 0.00576 | valid_mae: 0.05895 | valid_mse: 0.00735 |  0:00:16s\n",
      "epoch 33 | loss: 0.00533 | train_mae: 0.06266 | train_mse: 0.00636 | valid_mae: 0.06619 | valid_mse: 0.00789 |  0:00:16s\n",
      "epoch 34 | loss: 0.00549 | train_mae: 0.05954 | train_mse: 0.00604 | valid_mae: 0.06404 | valid_mse: 0.00782 |  0:00:16s\n",
      "epoch 35 | loss: 0.00506 | train_mae: 0.05046 | train_mse: 0.00537 | valid_mae: 0.05393 | valid_mse: 0.00691 |  0:00:17s\n",
      "epoch 36 | loss: 0.00537 | train_mae: 0.04999 | train_mse: 0.0053  | valid_mae: 0.05236 | valid_mse: 0.0067  |  0:00:17s\n",
      "epoch 37 | loss: 0.00509 | train_mae: 0.05299 | train_mse: 0.00554 | valid_mae: 0.05432 | valid_mse: 0.0069  |  0:00:18s\n",
      "epoch 38 | loss: 0.00498 | train_mae: 0.04999 | train_mse: 0.00528 | valid_mae: 0.052   | valid_mse: 0.00669 |  0:00:18s\n",
      "epoch 39 | loss: 0.00524 | train_mae: 0.04628 | train_mse: 0.00502 | valid_mae: 0.04939 | valid_mse: 0.00652 |  0:00:19s\n",
      "epoch 40 | loss: 0.00515 | train_mae: 0.04746 | train_mse: 0.00505 | valid_mae: 0.05042 | valid_mse: 0.00644 |  0:00:19s\n",
      "epoch 41 | loss: 0.00496 | train_mae: 0.04722 | train_mse: 0.00497 | valid_mae: 0.05009 | valid_mse: 0.00628 |  0:00:20s\n",
      "epoch 42 | loss: 0.00488 | train_mae: 0.04583 | train_mse: 0.00486 | valid_mae: 0.0497  | valid_mse: 0.00633 |  0:00:20s\n",
      "epoch 43 | loss: 0.00471 | train_mae: 0.0448  | train_mse: 0.00472 | valid_mae: 0.04948 | valid_mse: 0.0063  |  0:00:21s\n",
      "epoch 44 | loss: 0.00473 | train_mae: 0.04469 | train_mse: 0.00472 | valid_mae: 0.04898 | valid_mse: 0.00628 |  0:00:21s\n",
      "epoch 45 | loss: 0.00484 | train_mae: 0.04441 | train_mse: 0.00467 | valid_mae: 0.04982 | valid_mse: 0.00635 |  0:00:22s\n",
      "epoch 46 | loss: 0.00465 | train_mae: 0.04441 | train_mse: 0.00464 | valid_mae: 0.04936 | valid_mse: 0.00621 |  0:00:22s\n",
      "epoch 47 | loss: 0.00453 | train_mae: 0.04533 | train_mse: 0.00463 | valid_mae: 0.04891 | valid_mse: 0.00584 |  0:00:22s\n",
      "epoch 48 | loss: 0.00462 | train_mae: 0.04483 | train_mse: 0.0048  | valid_mae: 0.0489  | valid_mse: 0.0063  |  0:00:23s\n",
      "epoch 49 | loss: 0.00458 | train_mae: 0.04621 | train_mse: 0.00509 | valid_mae: 0.05053 | valid_mse: 0.00667 |  0:00:23s\n",
      "epoch 50 | loss: 0.00465 | train_mae: 0.04939 | train_mse: 0.00534 | valid_mae: 0.05487 | valid_mse: 0.00705 |  0:00:24s\n",
      "epoch 51 | loss: 0.00472 | train_mae: 0.04975 | train_mse: 0.00552 | valid_mae: 0.05694 | valid_mse: 0.00772 |  0:00:24s\n",
      "epoch 52 | loss: 0.00483 | train_mae: 0.05074 | train_mse: 0.00542 | valid_mae: 0.05679 | valid_mse: 0.00724 |  0:00:25s\n",
      "epoch 53 | loss: 0.00472 | train_mae: 0.0499  | train_mse: 0.00545 | valid_mae: 0.05499 | valid_mse: 0.00739 |  0:00:25s\n",
      "epoch 54 | loss: 0.00466 | train_mae: 0.04963 | train_mse: 0.00543 | valid_mae: 0.05522 | valid_mse: 0.00753 |  0:00:26s\n",
      "epoch 55 | loss: 0.00449 | train_mae: 0.05123 | train_mse: 0.00546 | valid_mae: 0.05635 | valid_mse: 0.00766 |  0:00:26s\n",
      "epoch 56 | loss: 0.00451 | train_mae: 0.05057 | train_mse: 0.00542 | valid_mae: 0.05666 | valid_mse: 0.00763 |  0:00:26s\n",
      "epoch 57 | loss: 0.00446 | train_mae: 0.04967 | train_mse: 0.00516 | valid_mae: 0.05653 | valid_mse: 0.00738 |  0:00:27s\n",
      "epoch 58 | loss: 0.00441 | train_mae: 0.04976 | train_mse: 0.00539 | valid_mae: 0.05557 | valid_mse: 0.00738 |  0:00:27s\n",
      "epoch 59 | loss: 0.00435 | train_mae: 0.05048 | train_mse: 0.00535 | valid_mae: 0.0568  | valid_mse: 0.00763 |  0:00:28s\n",
      "epoch 60 | loss: 0.00428 | train_mae: 0.05135 | train_mse: 0.00517 | valid_mae: 0.05685 | valid_mse: 0.00737 |  0:00:28s\n",
      "epoch 61 | loss: 0.00415 | train_mae: 0.0471  | train_mse: 0.00488 | valid_mae: 0.05377 | valid_mse: 0.00748 |  0:00:29s\n",
      "epoch 62 | loss: 0.00438 | train_mae: 0.04679 | train_mse: 0.00467 | valid_mae: 0.05433 | valid_mse: 0.00712 |  0:00:29s\n",
      "epoch 63 | loss: 0.0042  | train_mae: 0.04955 | train_mse: 0.0046  | valid_mae: 0.05605 | valid_mse: 0.00654 |  0:00:30s\n",
      "epoch 64 | loss: 0.00421 | train_mae: 0.04616 | train_mse: 0.00436 | valid_mae: 0.05242 | valid_mse: 0.00627 |  0:00:30s\n",
      "epoch 65 | loss: 0.00411 | train_mae: 0.04537 | train_mse: 0.00433 | valid_mae: 0.05212 | valid_mse: 0.00609 |  0:00:30s\n",
      "epoch 66 | loss: 0.00413 | train_mae: 0.04791 | train_mse: 0.00442 | valid_mae: 0.0548  | valid_mse: 0.0062  |  0:00:31s\n",
      "epoch 67 | loss: 0.00413 | train_mae: 0.04408 | train_mse: 0.00413 | valid_mae: 0.05184 | valid_mse: 0.00605 |  0:00:31s\n",
      "epoch 68 | loss: 0.00414 | train_mae: 0.04277 | train_mse: 0.00409 | valid_mae: 0.0503  | valid_mse: 0.00596 |  0:00:32s\n",
      "epoch 69 | loss: 0.0041  | train_mae: 0.04887 | train_mse: 0.00446 | valid_mae: 0.05622 | valid_mse: 0.00637 |  0:00:32s\n",
      "epoch 70 | loss: 0.00399 | train_mae: 0.04474 | train_mse: 0.00426 | valid_mae: 0.05409 | valid_mse: 0.00645 |  0:00:33s\n",
      "epoch 71 | loss: 0.00403 | train_mae: 0.04435 | train_mse: 0.0042  | valid_mae: 0.05449 | valid_mse: 0.00654 |  0:00:33s\n",
      "epoch 72 | loss: 0.00398 | train_mae: 0.04728 | train_mse: 0.00432 | valid_mae: 0.05821 | valid_mse: 0.00682 |  0:00:34s\n",
      "epoch 73 | loss: 0.00424 | train_mae: 0.04275 | train_mse: 0.00417 | valid_mae: 0.0543  | valid_mse: 0.00683 |  0:00:34s\n",
      "epoch 74 | loss: 0.00405 | train_mae: 0.04499 | train_mse: 0.00416 | valid_mae: 0.05689 | valid_mse: 0.00688 |  0:00:35s\n",
      "epoch 75 | loss: 0.00366 | train_mae: 0.04686 | train_mse: 0.00422 | valid_mae: 0.05805 | valid_mse: 0.00681 |  0:00:35s\n",
      "epoch 76 | loss: 0.00377 | train_mae: 0.04246 | train_mse: 0.00398 | valid_mae: 0.05379 | valid_mse: 0.00637 |  0:00:36s\n",
      "epoch 77 | loss: 0.00392 | train_mae: 0.04536 | train_mse: 0.00401 | valid_mae: 0.05562 | valid_mse: 0.00622 |  0:00:36s\n",
      "epoch 78 | loss: 0.00385 | train_mae: 0.04329 | train_mse: 0.00388 | valid_mae: 0.05371 | valid_mse: 0.00598 |  0:00:36s\n",
      "epoch 79 | loss: 0.00372 | train_mae: 0.04223 | train_mse: 0.00379 | valid_mae: 0.05253 | valid_mse: 0.00592 |  0:00:37s\n",
      "epoch 80 | loss: 0.00383 | train_mae: 0.04646 | train_mse: 0.00398 | valid_mae: 0.05573 | valid_mse: 0.00611 |  0:00:37s\n",
      "epoch 81 | loss: 0.00397 | train_mae: 0.04147 | train_mse: 0.00381 | valid_mae: 0.0504  | valid_mse: 0.0058  |  0:00:38s\n",
      "epoch 82 | loss: 0.00374 | train_mae: 0.04139 | train_mse: 0.00376 | valid_mae: 0.04935 | valid_mse: 0.00558 |  0:00:38s\n",
      "epoch 83 | loss: 0.00374 | train_mae: 0.04739 | train_mse: 0.00407 | valid_mae: 0.05174 | valid_mse: 0.00556 |  0:00:39s\n",
      "epoch 84 | loss: 0.00382 | train_mae: 0.04104 | train_mse: 0.00363 | valid_mae: 0.04615 | valid_mse: 0.00517 |  0:00:39s\n",
      "epoch 85 | loss: 0.00364 | train_mae: 0.04163 | train_mse: 0.00389 | valid_mae: 0.0471  | valid_mse: 0.00544 |  0:00:40s\n",
      "epoch 86 | loss: 0.00369 | train_mae: 0.04329 | train_mse: 0.00373 | valid_mae: 0.04413 | valid_mse: 0.00458 |  0:00:40s\n",
      "epoch 87 | loss: 0.0036  | train_mae: 0.04197 | train_mse: 0.00381 | valid_mae: 0.04296 | valid_mse: 0.00476 |  0:00:41s\n",
      "epoch 88 | loss: 0.00338 | train_mae: 0.0424  | train_mse: 0.00402 | valid_mae: 0.04611 | valid_mse: 0.00554 |  0:00:41s\n",
      "epoch 89 | loss: 0.00355 | train_mae: 0.04287 | train_mse: 0.00378 | valid_mae: 0.04577 | valid_mse: 0.00546 |  0:00:41s\n",
      "epoch 90 | loss: 0.00373 | train_mae: 0.04124 | train_mse: 0.00359 | valid_mae: 0.04618 | valid_mse: 0.00539 |  0:00:42s\n",
      "epoch 91 | loss: 0.00349 | train_mae: 0.04108 | train_mse: 0.00368 | valid_mae: 0.04699 | valid_mse: 0.00566 |  0:00:42s\n",
      "epoch 92 | loss: 0.00337 | train_mae: 0.04339 | train_mse: 0.00372 | valid_mae: 0.04996 | valid_mse: 0.00571 |  0:00:43s\n",
      "epoch 93 | loss: 0.00352 | train_mae: 0.04196 | train_mse: 0.0036  | valid_mae: 0.04864 | valid_mse: 0.00563 |  0:00:43s\n",
      "epoch 94 | loss: 0.0036  | train_mae: 0.04115 | train_mse: 0.00371 | valid_mae: 0.04795 | valid_mse: 0.00578 |  0:00:44s\n",
      "epoch 95 | loss: 0.00363 | train_mae: 0.04134 | train_mse: 0.00355 | valid_mae: 0.04673 | valid_mse: 0.00513 |  0:00:44s\n",
      "epoch 96 | loss: 0.00322 | train_mae: 0.04409 | train_mse: 0.00377 | valid_mae: 0.04722 | valid_mse: 0.00498 |  0:00:45s\n",
      "epoch 97 | loss: 0.00363 | train_mae: 0.04169 | train_mse: 0.00379 | valid_mae: 0.04761 | valid_mse: 0.00577 |  0:00:45s\n",
      "epoch 98 | loss: 0.00354 | train_mae: 0.04255 | train_mse: 0.00378 | valid_mae: 0.04672 | valid_mse: 0.00512 |  0:00:45s\n",
      "epoch 99 | loss: 0.00341 | train_mae: 0.04227 | train_mse: 0.00381 | valid_mae: 0.04637 | valid_mse: 0.00501 |  0:00:46s\n",
      "epoch 100| loss: 0.00333 | train_mae: 0.04268 | train_mse: 0.00392 | valid_mae: 0.04865 | valid_mse: 0.00573 |  0:00:46s\n",
      "epoch 101| loss: 0.00364 | train_mae: 0.03965 | train_mse: 0.00327 | valid_mae: 0.0417  | valid_mse: 0.00463 |  0:00:47s\n",
      "epoch 102| loss: 0.00316 | train_mae: 0.04128 | train_mse: 0.00332 | valid_mae: 0.04185 | valid_mse: 0.0046  |  0:00:47s\n",
      "epoch 103| loss: 0.00342 | train_mae: 0.04006 | train_mse: 0.00357 | valid_mae: 0.0478  | valid_mse: 0.00573 |  0:00:48s\n",
      "epoch 104| loss: 0.00367 | train_mae: 0.03837 | train_mse: 0.00319 | valid_mae: 0.04237 | valid_mse: 0.00494 |  0:00:48s\n",
      "epoch 105| loss: 0.0033  | train_mae: 0.04102 | train_mse: 0.00325 | valid_mae: 0.04262 | valid_mse: 0.00452 |  0:00:49s\n",
      "epoch 106| loss: 0.00338 | train_mae: 0.03893 | train_mse: 0.00324 | valid_mae: 0.0426  | valid_mse: 0.00486 |  0:00:49s\n",
      "epoch 107| loss: 0.00324 | train_mae: 0.03919 | train_mse: 0.00315 | valid_mae: 0.041   | valid_mse: 0.00451 |  0:00:49s\n",
      "epoch 108| loss: 0.00323 | train_mae: 0.04093 | train_mse: 0.00319 | valid_mae: 0.03975 | valid_mse: 0.00416 |  0:00:50s\n",
      "epoch 109| loss: 0.00312 | train_mae: 0.03963 | train_mse: 0.00332 | valid_mae: 0.04229 | valid_mse: 0.0048  |  0:00:50s\n",
      "epoch 110| loss: 0.00307 | train_mae: 0.03969 | train_mse: 0.00336 | valid_mae: 0.04362 | valid_mse: 0.00492 |  0:00:51s\n",
      "epoch 111| loss: 0.00301 | train_mae: 0.03957 | train_mse: 0.00329 | valid_mae: 0.03958 | valid_mse: 0.00438 |  0:00:51s\n",
      "epoch 112| loss: 0.00303 | train_mae: 0.03974 | train_mse: 0.00332 | valid_mae: 0.04072 | valid_mse: 0.00456 |  0:00:52s\n",
      "epoch 113| loss: 0.00306 | train_mae: 0.03989 | train_mse: 0.0033  | valid_mae: 0.04156 | valid_mse: 0.00458 |  0:00:52s\n",
      "epoch 114| loss: 0.00313 | train_mae: 0.04036 | train_mse: 0.0033  | valid_mae: 0.04056 | valid_mse: 0.00443 |  0:00:53s\n",
      "epoch 115| loss: 0.00308 | train_mae: 0.03942 | train_mse: 0.00344 | valid_mae: 0.04152 | valid_mse: 0.00475 |  0:00:53s\n",
      "epoch 116| loss: 0.00283 | train_mae: 0.03923 | train_mse: 0.00327 | valid_mae: 0.0392  | valid_mse: 0.00431 |  0:00:54s\n",
      "epoch 117| loss: 0.00278 | train_mae: 0.04042 | train_mse: 0.00333 | valid_mae: 0.03972 | valid_mse: 0.00405 |  0:00:54s\n",
      "epoch 118| loss: 0.00305 | train_mae: 0.03906 | train_mse: 0.00342 | valid_mae: 0.04019 | valid_mse: 0.00449 |  0:00:54s\n",
      "epoch 119| loss: 0.003   | train_mae: 0.03888 | train_mse: 0.00332 | valid_mae: 0.03857 | valid_mse: 0.0043  |  0:00:55s\n",
      "epoch 120| loss: 0.00285 | train_mae: 0.03933 | train_mse: 0.0032  | valid_mae: 0.03765 | valid_mse: 0.00415 |  0:00:55s\n",
      "epoch 121| loss: 0.00276 | train_mae: 0.03887 | train_mse: 0.0031  | valid_mae: 0.03767 | valid_mse: 0.00425 |  0:00:56s\n",
      "epoch 122| loss: 0.00283 | train_mae: 0.03984 | train_mse: 0.0031  | valid_mae: 0.03765 | valid_mse: 0.00406 |  0:00:56s\n",
      "epoch 123| loss: 0.00291 | train_mae: 0.03861 | train_mse: 0.00307 | valid_mae: 0.03798 | valid_mse: 0.0042  |  0:00:57s\n",
      "epoch 124| loss: 0.00276 | train_mae: 0.03794 | train_mse: 0.00314 | valid_mae: 0.03969 | valid_mse: 0.00457 |  0:00:57s\n",
      "epoch 125| loss: 0.00288 | train_mae: 0.0372  | train_mse: 0.00295 | valid_mae: 0.03841 | valid_mse: 0.00429 |  0:00:58s\n",
      "epoch 126| loss: 0.00269 | train_mae: 0.03744 | train_mse: 0.00286 | valid_mae: 0.03774 | valid_mse: 0.00416 |  0:00:58s\n",
      "epoch 127| loss: 0.0027  | train_mae: 0.03699 | train_mse: 0.00288 | valid_mae: 0.03836 | valid_mse: 0.00432 |  0:00:58s\n",
      "epoch 128| loss: 0.00269 | train_mae: 0.03678 | train_mse: 0.00292 | valid_mae: 0.03871 | valid_mse: 0.00443 |  0:00:59s\n",
      "epoch 129| loss: 0.0026  | train_mae: 0.03712 | train_mse: 0.00281 | valid_mae: 0.03767 | valid_mse: 0.00413 |  0:00:59s\n",
      "epoch 130| loss: 0.00292 | train_mae: 0.03658 | train_mse: 0.00287 | valid_mae: 0.03877 | valid_mse: 0.00446 |  0:01:00s\n",
      "epoch 131| loss: 0.00269 | train_mae: 0.03655 | train_mse: 0.00287 | valid_mae: 0.03814 | valid_mse: 0.00435 |  0:01:00s\n",
      "epoch 132| loss: 0.00275 | train_mae: 0.03729 | train_mse: 0.00279 | valid_mae: 0.03688 | valid_mse: 0.00401 |  0:01:01s\n",
      "epoch 133| loss: 0.00259 | train_mae: 0.03646 | train_mse: 0.00296 | valid_mae: 0.03847 | valid_mse: 0.00446 |  0:01:01s\n",
      "epoch 134| loss: 0.00284 | train_mae: 0.03616 | train_mse: 0.00291 | valid_mae: 0.03861 | valid_mse: 0.00442 |  0:01:02s\n",
      "epoch 135| loss: 0.00285 | train_mae: 0.03837 | train_mse: 0.00269 | valid_mae: 0.03629 | valid_mse: 0.00354 |  0:01:02s\n",
      "epoch 136| loss: 0.00294 | train_mae: 0.03547 | train_mse: 0.00261 | valid_mae: 0.03549 | valid_mse: 0.00377 |  0:01:02s\n",
      "epoch 137| loss: 0.00265 | train_mae: 0.0367  | train_mse: 0.00299 | valid_mae: 0.03977 | valid_mse: 0.00446 |  0:01:03s\n",
      "epoch 138| loss: 0.00276 | train_mae: 0.03648 | train_mse: 0.00266 | valid_mae: 0.03703 | valid_mse: 0.00378 |  0:01:03s\n",
      "epoch 139| loss: 0.00273 | train_mae: 0.03707 | train_mse: 0.00267 | valid_mae: 0.0373  | valid_mse: 0.00375 |  0:01:04s\n",
      "epoch 140| loss: 0.0027  | train_mae: 0.03607 | train_mse: 0.00276 | valid_mae: 0.03639 | valid_mse: 0.00404 |  0:01:04s\n",
      "epoch 141| loss: 0.00279 | train_mae: 0.03645 | train_mse: 0.00262 | valid_mae: 0.03498 | valid_mse: 0.00356 |  0:01:05s\n",
      "epoch 142| loss: 0.00251 | train_mae: 0.03721 | train_mse: 0.00264 | valid_mae: 0.03578 | valid_mse: 0.00349 |  0:01:05s\n",
      "epoch 143| loss: 0.00261 | train_mae: 0.03571 | train_mse: 0.00263 | valid_mae: 0.0348  | valid_mse: 0.00364 |  0:01:06s\n",
      "epoch 144| loss: 0.00274 | train_mae: 0.03605 | train_mse: 0.00259 | valid_mae: 0.03473 | valid_mse: 0.00353 |  0:01:06s\n",
      "epoch 145| loss: 0.00246 | train_mae: 0.03578 | train_mse: 0.00258 | valid_mae: 0.03552 | valid_mse: 0.00357 |  0:01:07s\n",
      "epoch 146| loss: 0.00273 | train_mae: 0.03518 | train_mse: 0.00257 | valid_mae: 0.03587 | valid_mse: 0.00367 |  0:01:07s\n",
      "epoch 147| loss: 0.00255 | train_mae: 0.03507 | train_mse: 0.00249 | valid_mae: 0.03533 | valid_mse: 0.00355 |  0:01:07s\n",
      "epoch 148| loss: 0.00243 | train_mae: 0.03431 | train_mse: 0.00257 | valid_mae: 0.0363  | valid_mse: 0.00393 |  0:01:08s\n",
      "epoch 149| loss: 0.00264 | train_mae: 0.03456 | train_mse: 0.00253 | valid_mae: 0.03861 | valid_mse: 0.00412 |  0:01:08s\n",
      "epoch 150| loss: 0.00273 | train_mae: 0.03501 | train_mse: 0.00264 | valid_mae: 0.03975 | valid_mse: 0.00478 |  0:01:09s\n",
      "epoch 151| loss: 0.00305 | train_mae: 0.03492 | train_mse: 0.00278 | valid_mae: 0.03923 | valid_mse: 0.00476 |  0:01:09s\n",
      "epoch 152| loss: 0.00317 | train_mae: 0.03612 | train_mse: 0.00294 | valid_mae: 0.03734 | valid_mse: 0.00439 |  0:01:10s\n",
      "epoch 153| loss: 0.00326 | train_mae: 0.03932 | train_mse: 0.00308 | valid_mae: 0.0396  | valid_mse: 0.00422 |  0:01:10s\n",
      "epoch 154| loss: 0.00346 | train_mae: 0.03761 | train_mse: 0.0031  | valid_mae: 0.03977 | valid_mse: 0.00436 |  0:01:11s\n",
      "epoch 155| loss: 0.0031  | train_mae: 0.03738 | train_mse: 0.00309 | valid_mae: 0.03937 | valid_mse: 0.00435 |  0:01:11s\n",
      "epoch 156| loss: 0.00323 | train_mae: 0.03772 | train_mse: 0.00291 | valid_mae: 0.03764 | valid_mse: 0.004   |  0:01:11s\n",
      "epoch 157| loss: 0.00303 | train_mae: 0.03728 | train_mse: 0.0028  | valid_mae: 0.03643 | valid_mse: 0.00384 |  0:01:12s\n",
      "epoch 158| loss: 0.00298 | train_mae: 0.03542 | train_mse: 0.00277 | valid_mae: 0.0386  | valid_mse: 0.00448 |  0:01:12s\n",
      "epoch 159| loss: 0.00307 | train_mae: 0.03499 | train_mse: 0.00261 | valid_mae: 0.03765 | valid_mse: 0.00442 |  0:01:13s\n",
      "epoch 160| loss: 0.0029  | train_mae: 0.0376  | train_mse: 0.00267 | valid_mae: 0.03868 | valid_mse: 0.0044  |  0:01:13s\n",
      "epoch 161| loss: 0.0031  | train_mae: 0.0353  | train_mse: 0.00271 | valid_mae: 0.03925 | valid_mse: 0.00483 |  0:01:14s\n",
      "epoch 162| loss: 0.00288 | train_mae: 0.03533 | train_mse: 0.00255 | valid_mae: 0.03684 | valid_mse: 0.00434 |  0:01:14s\n",
      "epoch 163| loss: 0.0029  | train_mae: 0.03674 | train_mse: 0.00257 | valid_mae: 0.03764 | valid_mse: 0.00439 |  0:01:15s\n",
      "epoch 164| loss: 0.00277 | train_mae: 0.03354 | train_mse: 0.00251 | valid_mae: 0.0363  | valid_mse: 0.00464 |  0:01:15s\n",
      "epoch 165| loss: 0.00287 | train_mae: 0.03297 | train_mse: 0.00241 | valid_mae: 0.0365  | valid_mse: 0.00453 |  0:01:15s\n",
      "epoch 166| loss: 0.00272 | train_mae: 0.03385 | train_mse: 0.00238 | valid_mae: 0.0373  | valid_mse: 0.00441 |  0:01:16s\n",
      "epoch 167| loss: 0.00269 | train_mae: 0.03334 | train_mse: 0.00237 | valid_mae: 0.03862 | valid_mse: 0.00461 |  0:01:16s\n",
      "epoch 168| loss: 0.00281 | train_mae: 0.0331  | train_mse: 0.00239 | valid_mae: 0.03818 | valid_mse: 0.00467 |  0:01:17s\n",
      "epoch 169| loss: 0.00276 | train_mae: 0.03311 | train_mse: 0.0023  | valid_mae: 0.03759 | valid_mse: 0.00447 |  0:01:17s\n",
      "epoch 170| loss: 0.00266 | train_mae: 0.03274 | train_mse: 0.00231 | valid_mae: 0.03761 | valid_mse: 0.00441 |  0:01:18s\n",
      "epoch 171| loss: 0.0028  | train_mae: 0.03305 | train_mse: 0.00228 | valid_mae: 0.03766 | valid_mse: 0.00423 |  0:01:18s\n",
      "epoch 172| loss: 0.00262 | train_mae: 0.03393 | train_mse: 0.00229 | valid_mae: 0.03784 | valid_mse: 0.00417 |  0:01:19s\n",
      "epoch 173| loss: 0.00272 | train_mae: 0.03363 | train_mse: 0.00233 | valid_mae: 0.03785 | valid_mse: 0.0044  |  0:01:19s\n",
      "epoch 174| loss: 0.00261 | train_mae: 0.03473 | train_mse: 0.00231 | valid_mae: 0.03971 | valid_mse: 0.00456 |  0:01:19s\n",
      "epoch 175| loss: 0.00268 | train_mae: 0.03318 | train_mse: 0.00224 | valid_mae: 0.03887 | valid_mse: 0.00455 |  0:01:20s\n",
      "epoch 176| loss: 0.00246 | train_mae: 0.03295 | train_mse: 0.00223 | valid_mae: 0.03848 | valid_mse: 0.00449 |  0:01:20s\n",
      "epoch 177| loss: 0.0025  | train_mae: 0.03496 | train_mse: 0.00229 | valid_mae: 0.03962 | valid_mse: 0.00442 |  0:01:21s\n",
      "epoch 178| loss: 0.00258 | train_mae: 0.03324 | train_mse: 0.00223 | valid_mae: 0.03798 | valid_mse: 0.00443 |  0:01:21s\n",
      "epoch 179| loss: 0.00256 | train_mae: 0.03351 | train_mse: 0.00239 | valid_mae: 0.03963 | valid_mse: 0.00473 |  0:01:22s\n",
      "epoch 180| loss: 0.00263 | train_mae: 0.03495 | train_mse: 0.00225 | valid_mae: 0.03997 | valid_mse: 0.0044  |  0:01:22s\n",
      "epoch 181| loss: 0.00267 | train_mae: 0.03308 | train_mse: 0.00213 | valid_mae: 0.03886 | valid_mse: 0.00443 |  0:01:23s\n",
      "epoch 182| loss: 0.0026  | train_mae: 0.0327  | train_mse: 0.00231 | valid_mae: 0.03992 | valid_mse: 0.00483 |  0:01:23s\n",
      "epoch 183| loss: 0.00273 | train_mae: 0.03217 | train_mse: 0.00206 | valid_mae: 0.03712 | valid_mse: 0.00449 |  0:01:23s\n",
      "epoch 184| loss: 0.00262 | train_mae: 0.03241 | train_mse: 0.00204 | valid_mae: 0.03786 | valid_mse: 0.00443 |  0:01:24s\n",
      "epoch 185| loss: 0.00238 | train_mae: 0.03199 | train_mse: 0.00206 | valid_mae: 0.0372  | valid_mse: 0.00445 |  0:01:25s\n",
      "epoch 186| loss: 0.00244 | train_mae: 0.03293 | train_mse: 0.00205 | valid_mae: 0.03739 | valid_mse: 0.00432 |  0:01:25s\n",
      "epoch 187| loss: 0.0026  | train_mae: 0.03423 | train_mse: 0.00213 | valid_mae: 0.03807 | valid_mse: 0.0043  |  0:01:26s\n",
      "epoch 188| loss: 0.0025  | train_mae: 0.03245 | train_mse: 0.00214 | valid_mae: 0.03638 | valid_mse: 0.00447 |  0:01:26s\n",
      "epoch 189| loss: 0.00268 | train_mae: 0.03259 | train_mse: 0.00213 | valid_mae: 0.03664 | valid_mse: 0.00441 |  0:01:27s\n",
      "epoch 190| loss: 0.00243 | train_mae: 0.03535 | train_mse: 0.00222 | valid_mae: 0.03933 | valid_mse: 0.00441 |  0:01:27s\n",
      "epoch 191| loss: 0.00263 | train_mae: 0.0322  | train_mse: 0.00203 | valid_mae: 0.03691 | valid_mse: 0.00435 |  0:01:28s\n",
      "epoch 192| loss: 0.00241 | train_mae: 0.03199 | train_mse: 0.00206 | valid_mae: 0.03729 | valid_mse: 0.00457 |  0:01:28s\n",
      "\n",
      "Early stopping occurred at epoch 192 with best_epoch = 142 and best_valid_mse = 0.00349\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    }
   ],
   "source": [
    "clf.fit(\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "    eval_name=['train', 'valid'],\n",
    "    eval_metric=['mae', 'mse'],\n",
    "    max_epochs=max_epochs,\n",
    "    patience=50,\n",
    "    batch_size=1024, virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    drop_last=False,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST VALID SCORE: 0.0034919706980819673\n",
      "FINAL TEST SCORE: 0.0023986513055675486\n"
     ]
    },
    {
     "data": {
      "text/plain": "37511.12926712762"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = clf.predict(X_test)\n",
    "\n",
    "y_true = y_test\n",
    "\n",
    "test_score = mean_squared_error(y_pred=preds, y_true=y_true)\n",
    "\n",
    "print(f\"BEST VALID SCORE: {clf.best_cost}\")\n",
    "print(f\"FINAL TEST SCORE: {test_score}\")\n",
    "\n",
    "np.mean(np.abs(y_true-preds)*1000000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Result is better, than result showed by simple nn, but this result is still long way from the result shown by wooden models."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}